{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9f3c8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "with open('train.json', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    train = json.load(file)\n",
    "# Since it's too computationally expensive to run this model I won't be doing any validation testing.\n",
    "# with open('val.json', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "#     val = json.load(file)\n",
    "\n",
    "with open('test.json', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    test = json.load(file)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_train = pd.DataFrame(train)\n",
    "# df_val = pd.DataFrame(val)\n",
    "df_test = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5701864d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                            summary  \\\n",
      "0  13818513  Amanda baked cookies and will bring Jerry some...   \n",
      "1  13728867  Olivia and Olivier are voting for liberals in ...   \n",
      "2  13681000  Kim may try the pomodoro technique recommended...   \n",
      "3  13730747  Edward thinks he is in love with Bella. Rachel...   \n",
      "4  13728094  Sam is confused, because he overheard Rick com...   \n",
      "\n",
      "                                            dialogue  \n",
      "0  Amanda: I baked  cookies. Do you want some?\\r\\...  \n",
      "1  Olivia: Who are you voting for in this electio...  \n",
      "2  Tim: Hi, what's up?\\r\\nKim: Bad mood tbh, I wa...  \n",
      "3  Edward: Rachel, I think I'm in ove with Bella....  \n",
      "4  Sam: hey  overheard rick say something\\r\\nSam:...  \n",
      "         id                                            summary  \\\n",
      "0  13862856  Hannah needs Betty's number but Amanda doesn't...   \n",
      "1  13729565  Eric and Rob are going to watch a stand-up on ...   \n",
      "2  13680171  Lenny can't decide which trousers to buy. Bob ...   \n",
      "3  13729438  Emma will be home soon and she will let Will k...   \n",
      "4  13828600  Jane is in Warsaw. Ollie and Jane has a party....   \n",
      "\n",
      "                                            dialogue  \n",
      "0  Hannah: Hey, do you have Betty's number?\\nAman...  \n",
      "1  Eric: MACHINE!\\r\\nRob: That's so gr8!\\r\\nEric:...  \n",
      "2  Lenny: Babe, can you help me with something?\\r...  \n",
      "3  Will: hey babe, what do you want for dinner to...  \n",
      "4  Ollie: Hi , are you in Warsaw\\r\\nJane: yes, ju...  \n"
     ]
    }
   ],
   "source": [
    "print(df_train.head())\n",
    "# print(df_val.head())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "116be95f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7645e924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc6f2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the dataset class for conversations\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, dialogues, summaries, tokenizer, max_input_length=1024, max_target_length=150):\n",
    "        self.dialogues = dialogues\n",
    "        self.summaries = summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dialogues)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the dialogue and summary for the given index\n",
    "        dialogue = self.dialogues[idx]\n",
    "        summary = self.summaries[idx]\n",
    "\n",
    "        # Tokenize the dialogue and summary\n",
    "        input_encodings = self.tokenizer(dialogue,\n",
    "                                          max_length=self.max_input_length,\n",
    "                                          truncation=True,\n",
    "                                          padding='max_length',\n",
    "                                          return_tensors=\"pt\")\n",
    "        \n",
    "        target_encodings = self.tokenizer(summary,\n",
    "                                           max_length=self.max_target_length,\n",
    "                                           truncation=True,\n",
    "                                           padding='max_length',\n",
    "                                           return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_encodings['input_ids'].squeeze(0),  # Remove the batch dimension\n",
    "            'attention_mask': input_encodings['attention_mask'].squeeze(0),\n",
    "            'labels': target_encodings['input_ids'].squeeze(0)  # Ensure correct shape\n",
    "        }\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model_name = \"facebook/bart-base\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Prepare the dataset\n",
    "train_dataset = ConversationDataset(df_train['dialogue'].tolist(), df_train['summary'].tolist(), tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "# Move the model to the appropriate device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the training function\n",
    "def train_model(model, epochs=3, lr=0.0001):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=6, shuffle=True)\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Clear memory cache after each batch\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch: {epoch + 1}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13be1d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "GPU device name: NVIDIA GeForce RTX 2050\n"
     ]
    }
   ],
   "source": [
    "# Move model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU device name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3052483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akhba\\AppData\\Local\\Temp\\ipykernel_11024\\2114334750.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Epoch 1:   0%|                                                                                | 0/2456 [00:00<?, ?it/s]C:\\Users\\Akhba\\AppData\\Local\\Temp\\ipykernel_11024\\2114334750.py:68: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████████| 2456/2456 [2:57:12<00:00,  4.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 0.4605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|███████████████████████████████████████████████████████████████████| 2456/2456 [3:01:18<00:00,  4.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Average Loss: 0.3047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|███████████████████████████████████████████████████████████████████| 2456/2456 [3:01:45<00:00,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Average Loss: 0.2578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bded1a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "# model.save_pretrained(\"save/model3\")\n",
    "# tokenizer.save_pretrained(\"save/model3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f9c227c-61de-4bae-81a3-6a724b5420d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "\n",
    "# !pip install evaluate absl-py nltk rouge_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd3589ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████████████████████████████████████████████████████████████████████| 50/50 [03:48<00:00,  4.57s/it]\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "df_train = df_train.sample(n=300, random_state=42)\n",
    "# Define the test dataset and dataloader\n",
    "test_dataset = ConversationDataset(df_train['dialogue'].tolist(), df_train['summary'].tolist(), tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=6, shuffle=False)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "# Generate predictions and collect references\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        # Generate predictions\n",
    "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=150, num_beams=4)\n",
    "        \n",
    "        # Decode predictions and references\n",
    "        decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        decoded_refs = tokenizer.batch_decode(batch['labels'], skip_special_tokens=True)\n",
    "\n",
    "        predictions.extend(decoded_preds)\n",
    "        references.extend(decoded_refs)\n",
    "\n",
    "# Compute ROUGE scores\n",
    "results_training = rouge.compute(predictions=predictions, references=references, use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38c366aa-8a6f-454e-ac0d-f85ed403aea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|███████████████████████████████████████████████████████████████████████| 137/137 [11:28<00:00,  5.03s/it]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = ConversationDataset(df_test['dialogue'].tolist(), df_test['summary'].tolist(), tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=6, shuffle=False)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "# Generate predictions and collect references\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        # Generate predictions\n",
    "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=150, num_beams=4)\n",
    "        \n",
    "        # Decode predictions and references\n",
    "        decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        decoded_refs = tokenizer.batch_decode(batch['labels'], skip_special_tokens=True)\n",
    "\n",
    "        predictions.extend(decoded_preds)\n",
    "        references.extend(decoded_refs)\n",
    "\n",
    "# Compute ROUGE scores\n",
    "results_test = rouge.compute(predictions=predictions, references=references, use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6241a68e-9dbc-4115-a963-e666578ec564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: 0.5737\n",
      "rouge2: 0.3470\n",
      "rougeL: 0.4969\n",
      "rougeLsum: 0.4973\n"
     ]
    }
   ],
   "source": [
    "# Testing on the training dataset\n",
    "for key, value in results_training.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eb7a552-fd73-4bd5-bc38-0913e71df2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: 0.4969\n",
      "rouge2: 0.2462\n",
      "rougeL: 0.4081\n",
      "rougeLsum: 0.4080\n"
     ]
    }
   ],
   "source": [
    "# Testing on the test dataset\n",
    "for key, value in results_test.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8d01cbd-ae98-41bf-992d-31173c9b30a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Conversation 1 \n",
      "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
      "B: I’m pretty sure I am. What’s up?\n",
      "A: Can you go with me to the animal shelter?.\n",
      "B: What do you want to do?\n",
      "A: I want to get a puppy for my son.\n",
      "B: That will make him so happy.\n",
      "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
      "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-) \n",
      "A: I'll get him one of those little dogs.\n",
      "B: One that won't grow up too big;-)\n",
      "A: And eat too much;-))\n",
      "B: Do you know which one he would like?\n",
      "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
      "B: I bet you had to drag him away.\n",
      "A: He wanted to take it home right away ;-).\n",
      "B: I wonder what he'll name it.\n",
      "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
      "\n",
      " Conversation 2 \n",
      "Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids!\n",
      "Rob: I used to get one every year as a child! Loved them! \n",
      "Emma: Yeah, i remember! they were filled with chocolates!\n",
      "Lauren: they are different these days! much more sophisticated! Haha!\n",
      "Rob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff\n",
      "Emma: what do you fit inside?\n",
      "Lauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets\n",
      "Emma: WOW! That’s brill! X\n",
      "Lauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else\n",
      "Rob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc\n",
      "Lauren: i reckon it prepares them for Christmas \n",
      "Emma: and makes it more about traditions and being kind to other people\n",
      "Lauren: my children get very excited every time they get one!\n",
      "Emma: i can see why! :)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conversations = [\n",
    "    \"A: Hi Tom, are you busy tomorrow’s afternoon?\\r\\nB: I’m pretty sure I am. What’s up?\\r\\nA: Can you go with me to the animal shelter?.\\r\\nB: What do you want to do?\\r\\nA: I want to get a puppy for my son.\\r\\nB: That will make him so happy.\\r\\nA: Yeah, we’ve discussed it many times. I think he’s ready now.\\r\\nB: That’s good. Raising a dog is a tough issue. Like having a baby ;-) \\r\\nA: I'll get him one of those little dogs.\\r\\nB: One that won't grow up too big;-)\\r\\nA: And eat too much;-))\\r\\nB: Do you know which one he would like?\\r\\nA: Oh, yes, I took him there last Monday. He showed me one that he really liked.\\r\\nB: I bet you had to drag him away.\\r\\nA: He wanted to take it home right away ;-).\\r\\nB: I wonder what he'll name it.\\r\\nA: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\",\n",
    "    \"Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids!\\r\\nRob: I used to get one every year as a child! Loved them! \\r\\nEmma: Yeah, i remember! they were filled with chocolates!\\r\\nLauren: they are different these days! much more sophisticated! Haha!\\r\\nRob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff\\r\\nEmma: what do you fit inside?\\r\\nLauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets\\r\\nEmma: WOW! That’s brill! X\\r\\nLauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else\\r\\nRob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc\\r\\nLauren: i reckon it prepares them for Christmas \\r\\nEmma: and makes it more about traditions and being kind to other people\\r\\nLauren: my children get very excited every time they get one!\\r\\nEmma: i can see why! :)\",\n",
    "]\n",
    "\n",
    "# Tokenize and generate summaries\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "summaries = []\n",
    "for convo in conversations:\n",
    "    inputs = tokenizer(convo, max_length=1024, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=150, early_stopping=True)\n",
    "    # Decode the summary and add it to the list\n",
    "    summaries.append(tokenizer.decode(summary_ids[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "for i, conversation in enumerate(conversations):\n",
    "    print(f\" Conversation {i + 1} \\n{conversation}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ab25026-70b0-40fb-8860-17d923af69ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------SUMMARY RESULTS------\n",
      "Conversation 1 \n",
      " Summary: B will go with A to the animal shelter tomorrow afternoon. A wants to get a puppy for her son. A took him there last Monday. He liked the one that he liked. He will name it Lemmy after his dead hamster. \n",
      "\n",
      "Conversation 2 \n",
      " Summary: Emma wants an advent calendar for her kids. Rob used to get one every year as a child. Lauren adds notes for her children. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"------SUMMARY RESULTS------\")\n",
    "for i, summary in enumerate(summaries):\n",
    "    print(f\"Conversation {i + 1} \\n Summary: {summary} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66441e8a-4dac-4f1c-9d05-255af0f48c44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
