{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2692523c-9afc-4f5c-99f1-c3ff601c4050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b907c791-1c1f-42c1-b2be-d928799282ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BartConfig(\n",
    "    vocab_size=50265,\n",
    "    encoder_layers=4,  # Number of encoder layers\n",
    "    decoder_layers=4,  # Number of decoder layers\n",
    "    d_model=256,       # Dimensionality of the model\n",
    "    decoder_ffn_dim=1024,  \n",
    "    encoder_ffn_dim=1024,\n",
    "    max_position_embeddings=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b081a47-be6e-43bb-b785-9a6946e1e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartForConditionalGeneration(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2420fc4b-f96e-4de3-ba99-e5fec1264851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(model):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.reset_parameters()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.reset_parameters()\n",
    "reset_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1474b0dc-7703-49eb-82fd-c9136cfe4f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "GPU device name: NVIDIA GeForce RTX 2050\n"
     ]
    }
   ],
   "source": [
    "# Move model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU device name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97fe0e41-1313-4781-b704-2e0c5f35054f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): BartScaledWordEmbedding(50265, 256, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 256, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(514, 256)\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x BartEncoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 256, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(514, 256)\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x BartDecoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=256, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ed3872b-7c8e-458f-b303-635418aeb536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset class for conversations\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, dialogues, summaries, tokenizer, max_input_length=512, max_target_length=150):\n",
    "        self.dialogues = dialogues\n",
    "        self.summaries = summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dialogues)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the dialogue and summary for the given index\n",
    "        dialogue = self.dialogues[idx]\n",
    "        summary = self.summaries[idx]\n",
    "\n",
    "        # Tokenize the dialogue and summary\n",
    "        input_encodings = self.tokenizer(\n",
    "            dialogue,\n",
    "            max_length=self.max_input_length,  # Adjusted to match `max_position_embeddings`\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        target_encodings = self.tokenizer(\n",
    "            summary,\n",
    "            max_length=self.max_target_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_encodings['input_ids'].squeeze(0),  # Remove the batch dimension\n",
    "            'attention_mask': input_encodings['attention_mask'].squeeze(0),\n",
    "            'labels': target_encodings['input_ids'].squeeze(0)  # Ensure correct shape\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ee59b93-5650-4d51-968a-b5e3be00ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training function\n",
    "def train_model(model, epochs=3, lr=0.0001):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=6, shuffle=True)\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Clear memory cache after each batch\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch: {epoch + 1}, Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96e2bb9a-2d37-4c9d-a351-dac2720b1e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Gigaword dataset with custom code execution enabled\n",
    "dataset = load_dataset(\"gigaword\", trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "711f8b92-4838-4eef-8de2-6085d5c153df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akhba\\AppData\\Local\\Temp\\ipykernel_29948\\227675499.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "australia 's current account deficit shrunk by a record #.## billion dollars -lrb- #.## billion us -rrb- in the june quarter due to soaring commodity prices , figures released monday showed .\n",
      "australian current account deficit narrows sharply\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                                                                              | 0/116667 [00:00<?, ?it/s]C:\\Users\\Akhba\\AppData\\Local\\Temp\\ipykernel_29948\\227675499.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████| 116667/116667 [4:06:38<00:00,  7.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 0.3958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|███████████████████████████████████████████████████████████████| 116667/116667 [4:06:16<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Average Loss: 0.2561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|███████████████████████████████████████████████████████████████| 116667/116667 [4:06:26<00:00,  7.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Average Loss: 0.2107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "# Extract training data and limit to 700,000 samples\n",
    "train_data = dataset['train']\n",
    "train_dialogues = train_data['document'][:700000]\n",
    "train_summaries = train_data['summary'][:700000]\n",
    "\n",
    "print(train_dialogues[0])\n",
    "print(train_summaries[0])\n",
    "\n",
    "\n",
    "train_dialogues = [item for item in train_dialogues]\n",
    "train_summaries = [item for item in train_summaries]\n",
    "# val_dialogues = [item['document'] for item in val_data]\n",
    "# val_summaries = [item['summary'] for item in val_data]\n",
    "# Prepare the dataset\n",
    "train_dataset = ConversationDataset(train_dialogues, train_summaries, tokenizer)\n",
    "# Train the model\n",
    "train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ddc8cf8-6eb8-46ab-a101-3450ab6dd87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    train = json.load(file)\n",
    "# Since it's too computationally expensive to run this model I won't be doing any validation testing.\n",
    "# with open('val.json', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "#     val = json.load(file)\n",
    "\n",
    "with open('test.json', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    test = json.load(file)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_train = pd.DataFrame(train)\n",
    "# df_val = pd.DataFrame(val)\n",
    "df_test = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9e9407c-81d0-4455-96f7-5716fc6aa758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akhba\\AppData\\Local\\Temp\\ipykernel_29948\\227675499.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Epoch 1:   0%|                                                                                | 0/2456 [00:00<?, ?it/s]C:\\Users\\Akhba\\AppData\\Local\\Temp\\ipykernel_29948\\227675499.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1: 100%|█████████████████████████████████████████████████████████████████████| 2456/2456 [05:09<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 0.9571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|█████████████████████████████████████████████████████████████████████| 2456/2456 [05:05<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Average Loss: 0.8297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|█████████████████████████████████████████████████████████████████████| 2456/2456 [05:04<00:00,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Average Loss: 0.7730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fine tuning\n",
    "train_dataset = ConversationDataset(df_train['dialogue'].tolist(), df_train['summary'].tolist(), tokenizer)\n",
    "train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee7c283d-4555-4bae-8081-c6996f1a1144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Conversation 1 \n",
      "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
      "B: I’m pretty sure I am. What’s up?\n",
      "A: Can you go with me to the animal shelter?.\n",
      "B: What do you want to do?\n",
      "A: I want to get a puppy for my son.\n",
      "B: That will make him so happy.\n",
      "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
      "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-) \n",
      "A: I'll get him one of those little dogs.\n",
      "B: One that won't grow up too big;-)\n",
      "A: And eat too much;-))\n",
      "B: Do you know which one he would like?\n",
      "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
      "B: I bet you had to drag him away.\n",
      "A: He wanted to take it home right away ;-).\n",
      "B: I wonder what he'll name it.\n",
      "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
      "\n",
      " Conversation 2 \n",
      "Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids!\n",
      "Rob: I used to get one every year as a child! Loved them! \n",
      "Emma: Yeah, i remember! they were filled with chocolates!\n",
      "Lauren: they are different these days! much more sophisticated! Haha!\n",
      "Rob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff\n",
      "Emma: what do you fit inside?\n",
      "Lauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets\n",
      "Emma: WOW! That’s brill! X\n",
      "Lauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else\n",
      "Rob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc\n",
      "Lauren: i reckon it prepares them for Christmas \n",
      "Emma: and makes it more about traditions and being kind to other people\n",
      "Lauren: my children get very excited every time they get one!\n",
      "Emma: i can see why! :)\n",
      "\n",
      " Conversation 3 \n",
      "Behrouz: This model is very weak.\n",
      "Junaid: Yea bro I have to agree.\n",
      "Behrouz: So should we train another one and scrap this?\n",
      "Junaid: Nah It is what it is.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conversations = [\n",
    "    \"A: Hi Tom, are you busy tomorrow’s afternoon?\\r\\nB: I’m pretty sure I am. What’s up?\\r\\nA: Can you go with me to the animal shelter?.\\r\\nB: What do you want to do?\\r\\nA: I want to get a puppy for my son.\\r\\nB: That will make him so happy.\\r\\nA: Yeah, we’ve discussed it many times. I think he’s ready now.\\r\\nB: That’s good. Raising a dog is a tough issue. Like having a baby ;-) \\r\\nA: I'll get him one of those little dogs.\\r\\nB: One that won't grow up too big;-)\\r\\nA: And eat too much;-))\\r\\nB: Do you know which one he would like?\\r\\nA: Oh, yes, I took him there last Monday. He showed me one that he really liked.\\r\\nB: I bet you had to drag him away.\\r\\nA: He wanted to take it home right away ;-).\\r\\nB: I wonder what he'll name it.\\r\\nA: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\",\n",
    "    \"Emma: I’ve just fallen in love with this advent calendar! Awesome! I wanna one for my kids!\\r\\nRob: I used to get one every year as a child! Loved them! \\r\\nEmma: Yeah, i remember! they were filled with chocolates!\\r\\nLauren: they are different these days! much more sophisticated! Haha!\\r\\nRob: yeah, they can be fabric/ wooden, shop bought/ homemade, filled with various stuff\\r\\nEmma: what do you fit inside?\\r\\nLauren: small toys, Christmas decorations, creative stuff, hair bands & clips, stickers, pencils & rubbers, small puzzles, sweets\\r\\nEmma: WOW! That’s brill! X\\r\\nLauren: i add one more very special thing as well- little notes asking my children to do something nice for someone else\\r\\nRob: i like that! My sister adds notes asking her kids questions about christmas such as What did the 3 wise men bring? etc\\r\\nLauren: i reckon it prepares them for Christmas \\r\\nEmma: and makes it more about traditions and being kind to other people\\r\\nLauren: my children get very excited every time they get one!\\r\\nEmma: i can see why! :)\",\n",
    "    \"Behrouz: This model is very weak.\\nJunaid: Yea bro I have to agree.\\nBehrouz: So should we train another one and scrap this?\\nJunaid: Nah It is what it is.\"\n",
    "]\n",
    "\n",
    "# Tokenize and generate summaries\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "summaries = []\n",
    "for convo in conversations:\n",
    "    inputs = tokenizer(convo, max_length=1024, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=150, early_stopping=True)\n",
    "    # Decode the summary and add it to the list\n",
    "    summaries.append(tokenizer.decode(summary_ids[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "for i, conversation in enumerate(conversations):\n",
    "    print(f\" Conversation {i + 1} \\n{conversation}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2524ea6f-1f91-4748-b1bb-77daa09f7f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------SUMMARY RESULTS------\n",
      "Conversation 1 \n",
      " Summary: Anna is going to the animal tree. She is going to the restaurant. She is going to the store. \n",
      "\n",
      "Conversation 2 \n",
      " Summary: Emma is in love with this semester seats. She is going to get a baby and a baby for her children. \n",
      "\n",
      "Conversation 3 \n",
      " Summary: Eaid is very slow. She will be train another one. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"------SUMMARY RESULTS------\")\n",
    "for i, summary in enumerate(summaries):\n",
    "    print(f\"Conversation {i + 1} \\n Summary: {summary} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c719e28b-0d6c-4300-86f2-158365b6dc3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('save/model_reduced\\\\tokenizer_config.json',\n",
       " 'save/model_reduced\\\\special_tokens_map.json',\n",
       " 'save/model_reduced\\\\vocab.json',\n",
       " 'save/model_reduced\\\\merges.txt',\n",
       " 'save/model_reduced\\\\added_tokens.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"save/model_reduced\")\n",
    "tokenizer.save_pretrained(\"save/model_reduced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31c4363b-d6d2-471c-9ff1-3faae9f3f64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: Emarize is going to the gym.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Path to your saved model directory\n",
    "model_path = \"save/model_reduced\"  # Update with the correct directory path\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "# Test the model with a sample input\n",
    "input_text = \"Summarize this conversation for me.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# Generate a summary\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], max_length=150, num_beams=4, early_stopping=True)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd9ff385-d2dd-43c8-af47-0e0875acc52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████████████████████████████████████████████████████████████████████| 50/50 [01:11<00:00,  1.44s/it]\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "df_train = df_train.sample(n=300, random_state=42)\n",
    "# Define the test dataset and dataloader\n",
    "test_dataset = ConversationDataset(df_train['dialogue'].tolist(), df_train['summary'].tolist(), tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=6, shuffle=False)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "# Generate predictions and collect references\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "\n",
    "        # Generate predictions\n",
    "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=150, num_beams=4)\n",
    "        \n",
    "        # Decode predictions and references\n",
    "        decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        decoded_refs = tokenizer.batch_decode(batch['labels'], skip_special_tokens=True)\n",
    "\n",
    "        predictions.extend(decoded_preds)\n",
    "        references.extend(decoded_refs)\n",
    "\n",
    "# Compute ROUGE scores\n",
    "results_training = rouge.compute(predictions=predictions, references=references, use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40137d5b-00d9-4e07-90fb-7f939eee57e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: 0.2524\n",
      "rouge2: 0.0913\n",
      "rougeL: 0.2196\n",
      "rougeLsum: 0.2204\n"
     ]
    }
   ],
   "source": [
    "# Testing on the training dataset\n",
    "for key, value in results_training.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1391bc12-2855-4242-9045-6c3f31fd5e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|███████████████████████████████████████████████████████████████████████| 137/137 [03:20<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = ConversationDataset(df_test['dialogue'].tolist(), df_test['summary'].tolist(), tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=6, shuffle=False)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "# Generate predictions and collect references\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "\n",
    "        # Generate predictions\n",
    "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=150, num_beams=4)\n",
    "        \n",
    "        # Decode predictions and references\n",
    "        decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        decoded_refs = tokenizer.batch_decode(batch['labels'], skip_special_tokens=True)\n",
    "\n",
    "        predictions.extend(decoded_preds)\n",
    "        references.extend(decoded_refs)\n",
    "\n",
    "# Compute ROUGE scores\n",
    "results_test = rouge.compute(predictions=predictions, references=references, use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43fce5d5-3846-4779-91fb-3a60b11b3e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: 0.2334\n",
      "rouge2: 0.0731\n",
      "rougeL: 0.2029\n",
      "rougeLsum: 0.2030\n"
     ]
    }
   ],
   "source": [
    "# Testing on the test dataset\n",
    "for key, value in results_test.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f023a80-2adc-46fc-b810-5434de641d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
