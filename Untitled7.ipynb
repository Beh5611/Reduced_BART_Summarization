{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "efXXWGJEqs4G"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BartTokenizer, BartConfig\n",
    "\n",
    "class CustomBartEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CustomBartEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.encoder_layers = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=config.d_model, nhead=config.encoder_attention_heads\n",
    "            ),\n",
    "            num_layers=config.encoder_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        if attention_mask is not None:\n",
    "            # Convert to boolean tensor\n",
    "            attention_mask = attention_mask.bool()\n",
    "        output = self.encoder_layers(\n",
    "            embeddings.transpose(0, 1), src_key_padding_mask=attention_mask\n",
    "        )\n",
    "        return output.transpose(0, 1)\n",
    "\n",
    "\n",
    "class CustomBartDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CustomBartDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.decoder_layers = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(\n",
    "                d_model=config.d_model, nhead=config.decoder_attention_heads\n",
    "            ),\n",
    "            num_layers=config.decoder_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, encoder_output, attention_mask=None, encoder_attention_mask=None):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        if attention_mask is not None:\n",
    "            # Convert decoder attention mask to boolean\n",
    "            attention_mask = attention_mask.bool()\n",
    "        if encoder_attention_mask is not None:\n",
    "            # Convert encoder attention mask to boolean\n",
    "            encoder_attention_mask = encoder_attention_mask.bool()\n",
    "        output = self.decoder_layers(\n",
    "            embeddings.transpose(0, 1),\n",
    "            encoder_output.transpose(0, 1),\n",
    "            tgt_key_padding_mask=attention_mask,\n",
    "            memory_key_padding_mask=encoder_attention_mask\n",
    "        )\n",
    "        return output.transpose(0, 1)\n",
    "\n",
    "\n",
    "class CustomBartModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CustomBartModel, self).__init__()\n",
    "        self.encoder = CustomBartEncoder(config)\n",
    "        self.decoder = CustomBartDecoder(config)\n",
    "        self.linear = nn.Linear(config.d_model, config.vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, decoder_input_ids, attention_mask=None, decoder_attention_mask=None):\n",
    "        encoder_output = self.encoder(input_ids, attention_mask=attention_mask)\n",
    "        decoder_output = self.decoder(\n",
    "            decoder_input_ids,\n",
    "            encoder_output,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            encoder_attention_mask=attention_mask\n",
    "        )\n",
    "        logits = self.linear(decoder_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     custom_config = BartConfig(\n",
    "#       vocab_size=50265,          # Vocabulary size of the model\n",
    "#       d_model=768,               # Dimensionality of the encoder/decoder layers\n",
    "#       encoder_layers=4,          # Number of encoder layers\n",
    "#       decoder_layers=4,          # Number of decoder layers\n",
    "#       encoder_attention_heads=12, # Number of attention heads in encoder\n",
    "#       decoder_attention_heads=12, # Number of attention heads in decoder\n",
    "#       encoder_ffn_dim=3072,      # Feed-forward layer size in encoder\n",
    "#       decoder_ffn_dim=3072,      # Feed-forward layer size in decoder\n",
    "#       activation_function='gelu', # Activation function\n",
    "#       max_position_embeddings=1024, # Maximum sequence length\n",
    "#       dropout=0.1,               # Dropout rate\n",
    "#       attention_dropout=0.1,     # Dropout rate for attention weights\n",
    "#       use_cache=True             # Use cache during inference\n",
    "#   )\n",
    "#     run_inference(custom_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCGyMhyei8K2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nkHs87zRtxsL"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, dialogues, summaries, tokenizer, max_input_length=512, max_target_length=150):\n",
    "        self.dialogues = dialogues\n",
    "        self.summaries = summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dialogues)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the dialogue and summary for the given index\n",
    "        dialogue = self.dialogues[idx]\n",
    "        summary = self.summaries[idx]\n",
    "\n",
    "        # Tokenize the dialogue and summary\n",
    "        input_encodings = self.tokenizer(\n",
    "            dialogue,\n",
    "            max_length=150,  # Adjusted to match `max_position_embeddings`\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        target_encodings = self.tokenizer(\n",
    "            summary,\n",
    "            max_length=self.max_target_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_encodings['input_ids'].squeeze(0),  # Remove the batch dimension\n",
    "            'attention_mask': input_encodings['attention_mask'].squeeze(0),\n",
    "            'labels': target_encodings['input_ids'].squeeze(0)  # Ensure correct shape\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2IAaCTg5t5DM",
    "outputId": "4b7dfc68-3898-4e61-b3cd-6c018c115811"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2024.9.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (4.66.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\akhba\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['document', 'summary'],\n",
      "        num_rows: 3803957\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['document', 'summary'],\n",
      "        num_rows: 189651\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['document', 'summary'],\n",
      "        num_rows: 1951\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Gigaword dataset with custom code execution enabled\n",
    "dataset = load_dataset(\"gigaword\", trust_remote_code=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k7Ga_noguGaO",
    "outputId": "2a01a8e2-5d6c-4fba-af81-3fced2ce1c37"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akhba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|███████████████████████████████████████████████████████████████| 83334/83334 [3:18:37<00:00,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.7325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|███████████████████████████████████████████████████████████████| 83334/83334 [3:18:01<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.5333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|███████████████████████████████████████████████████████████████| 83334/83334 [3:18:03<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.3008\n",
      "Model training complete and saved!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BartTokenizer, BartConfig\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataset for sequence-to-sequence tasks\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, tokenizer, input_texts, target_texts, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_texts = input_texts\n",
    "        self.target_texts = target_texts\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.input_texts[idx]\n",
    "        target_text = self.target_texts[idx]\n",
    "\n",
    "        input_enc = self.tokenizer(\n",
    "            input_text, max_length=self.max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "        )\n",
    "        target_enc = self.tokenizer(\n",
    "            target_text, max_length=self.max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_enc.input_ids.squeeze(0),\n",
    "            \"attention_mask\": input_enc.attention_mask.squeeze(0),\n",
    "            \"labels\": target_enc.input_ids.squeeze(0)\n",
    "        }\n",
    "\n",
    "# Training function\n",
    "def train_model(model, dataloader, optimizer, tokenizer, num_epochs, device):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Shift decoder input ids for teacher forcing\n",
    "            decoder_input_ids = labels[:, :-1]\n",
    "            labels = labels[:, 1:].contiguous()\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids, decoder_input_ids, attention_mask)\n",
    "\n",
    "            # Compute loss\n",
    "            logits = logits.view(-1, logits.size(-1))\n",
    "            labels = labels.view(-1)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Loss: {epoch_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Script for training\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset\n",
    "    from datasets import load_dataset\n",
    "    # raw_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "    # Extract dialogues (articles) and summaries\n",
    "    # dialogues = raw_dataset['train']['article'][:200]\n",
    "    # summaries = raw_dataset['train']['highlights'][:200]\n",
    "    # Extract dialogues (news articles) and summaries\n",
    "    dialogues = dataset['train']['document'][:500000]  # The news articles\n",
    "\n",
    "    # print(dialogues)\n",
    "    summaries = dataset['train']['summary'][:500000] # The summaries\n",
    "    # Load the tokenizer\n",
    "\n",
    "    # Configuration and tokenizer\n",
    "    tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "    # custom_config = BartConfig(\n",
    "    #     vocab_size=50265,\n",
    "    #     d_model=768,\n",
    "    #     encoder_layers=4,\n",
    "    #     decoder_layers=4,\n",
    "    #     encoder_attention_heads=6,\n",
    "    #     decoder_attention_heads=6,\n",
    "    #     encoder_ffn_dim=1024,\n",
    "    #     decoder_ffn_dim=1024,\n",
    "    #     activation_function='gelu',\n",
    "    #     max_position_embeddings=1024,\n",
    "    #     use_cache=True\n",
    "    # )\n",
    "\n",
    "    custom_config = BartConfig(\n",
    "        vocab_size=50265,  # Adjust according to your tokenizer\n",
    "        encoder_layers=4,  # Number of encoder layers\n",
    "        decoder_layers=4,  # Number of decoder layers\n",
    "        d_model=256,       # Dimensionality of the model\n",
    "        decoder_ffn_dim=1024,  # FFN size\n",
    "        encoder_ffn_dim=1024,\n",
    "        max_position_embeddings=512\n",
    "    )\n",
    "\n",
    "    # Custom BART model\n",
    "    model = CustomBartModel(custom_config)\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Dataset parameters\n",
    "    max_input_length = 512\n",
    "    max_target_length = 150\n",
    "    dataset =  ConversationDataset(dialogues, summaries, tokenizer, max_input_length, max_target_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=6, shuffle=True)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "    # Training\n",
    "    device = torch.device(device)\n",
    "    train_model(model, dataloader, optimizer, tokenizer, num_epochs=3, device=device)\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), \"custom_bart_model.pth\")\n",
    "    print(\"Model training complete and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1WReiSDi91u",
    "outputId": "a9f3f3e0-4b76-412d-acc0-c30f2f9a5f66"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akhba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The quick brown fox jumps over the lazy dog. But he trips and lands on his face.\n",
      "Output:  peril toxic ginmone804 account diplomacy Register TRmonearious mantraFaithorks crawled804 account diplomacy Register stabilization dismantled upkeepBILL reduce TireAIDSarious mantra Santana faux Diagn Resist sky stabilization dismantled diplomacy faux Diagn Resist sky stabilization dismantled diplomacy faux Diagn Resist compuls TR 388BILL\n"
     ]
    }
   ],
   "source": [
    "def run_inference(custom_config, model_weights_path=\"custom_bart_model.pth\"):\n",
    "    # Load tokenizer and configuration\n",
    "    tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "    # Initialize custom model\n",
    "    model = CustomBartModel(custom_config)\n",
    "\n",
    "    # Load model weights\n",
    "    # model.load_state_dict(torch.load(model_weights_path))\n",
    "    model.eval()\n",
    "\n",
    "    # Input text\n",
    "    input_text = \"The quick brown fox jumps over the lazy dog. But he trips and lands on his face.\"\n",
    "    input_ids = tokenizer(input_text, return_tensors='pt').input_ids\n",
    "\n",
    "    # Prepare decoder input (start with BOS token)\n",
    "    decoder_input_ids = torch.tensor([[tokenizer.bos_token_id]])\n",
    "\n",
    "    # Iterative decoding\n",
    "    max_length = 50  # Define a reasonable maximum length\n",
    "    output_ids = decoder_input_ids\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, output_ids)\n",
    "        next_token_id = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(0)\n",
    "\n",
    "        # Break if EOS token is generated\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        # Append the new token to the sequence\n",
    "        output_ids = torch.cat([output_ids, next_token_id], dim=1)\n",
    "\n",
    "    # Decode the output tokens\n",
    "    output_text = tokenizer.decode(output_ids.squeeze(0), skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Input: {input_text}\")\n",
    "    print(f\"Output: {output_text}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    custom_config = BartConfig(\n",
    "        vocab_size=50265,  # Adjust according to your tokenizer\n",
    "        encoder_layers=4,  # Number of encoder layers\n",
    "        decoder_layers=4,  # Number of decoder layers\n",
    "        d_model=256,       # Dimensionality of the model\n",
    "        decoder_ffn_dim=1024,  # FFN size\n",
    "        encoder_ffn_dim=1024,\n",
    "        max_position_embeddings=512\n",
    "    )\n",
    "\n",
    "\n",
    "    run_inference(custom_config, model_weights_path=\"custom_bart_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "with open('train.json', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    train = json.load(file)\n",
    "# Since it's too computationally expensive to run this model I won't be doing any validation testing.\n",
    "# with open('val.json', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "#     val = json.load(file)\n",
    "\n",
    "with open('test.json', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    test = json.load(file)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_train = pd.DataFrame(train)\n",
    "# df_val = pd.DataFrame(val)\n",
    "df_test = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|███████████████████████████████████████████████████████████████████| 2456/2456 [05:56<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.5846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|███████████████████████████████████████████████████████████████████| 2456/2456 [05:55<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.2949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|███████████████████████████████████████████████████████████████████| 2456/2456 [05:55<00:00,  6.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.7698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fine tuning\n",
    "train_dataset = ConversationDataset(df_train['dialogue'].tolist(), df_train['summary'].tolist(), tokenizer)\n",
    "dataloader = DataLoader(train_dataset, batch_size=6, shuffle=True)\n",
    "train_model(model, dataloader, optimizer, tokenizer, num_epochs=3, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: A: Hi Tom, are you busy tomorrow’s afternoon?\n",
      "B: I’m pretty sure I am. What’s up?\n",
      "A: Can you go with me to the animal shelter?.\n",
      "B: What do you want to do?\n",
      "A: I want to get a puppy for my son.\n",
      "B: That will make him so happy.\n",
      "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
      "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-) \n",
      "A: I'll get him one of those little dogs.\n",
      "B: One that won't grow up too big;-)\n",
      "A: And eat too much;-))\n",
      "B: Do you know which one he would like?\n",
      "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
      "B: I bet you had to drag him away.\n",
      "A: He wanted to take it home right away ;-).\n",
      "B: I wonder what he'll name it.\n",
      "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
      "Output: IAL decency Sharing Britons vile Tribcrimcategory Fear veterin provocation convertingCatal invites Pierre undergoneivation extrContactIAL Tata stakeholders Fearicasagi dependencies convertingCatal invites Pierre undergoneivation extr strugg go accompIAL Tata stakeholders leasedissan 1 Nikeiances undergone Sharing accompIAL Tata stakeholders\n"
     ]
    }
   ],
   "source": [
    "def run_inference(custom_config, model_weights_path=\"custom_bart_model.pth\"):\n",
    "    # Load tokenizer and configuration\n",
    "    tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "    # Initialize custom model\n",
    "    model = CustomBartModel(custom_config)\n",
    "\n",
    "    # Load model weights\n",
    "    # model.load_state_dict(torch.load(model_weights_path))\n",
    "    model.eval()\n",
    "\n",
    "    # Input text\n",
    "    input_text = \"A: Hi Tom, are you busy tomorrow’s afternoon?\\r\\nB: I’m pretty sure I am. What’s up?\\r\\nA: Can you go with me to the animal shelter?.\\r\\nB: What do you want to do?\\r\\nA: I want to get a puppy for my son.\\r\\nB: That will make him so happy.\\r\\nA: Yeah, we’ve discussed it many times. I think he’s ready now.\\r\\nB: That’s good. Raising a dog is a tough issue. Like having a baby ;-) \\r\\nA: I'll get him one of those little dogs.\\r\\nB: One that won't grow up too big;-)\\r\\nA: And eat too much;-))\\r\\nB: Do you know which one he would like?\\r\\nA: Oh, yes, I took him there last Monday. He showed me one that he really liked.\\r\\nB: I bet you had to drag him away.\\r\\nA: He wanted to take it home right away ;-).\\r\\nB: I wonder what he'll name it.\\r\\nA: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\"\n",
    "    input_ids = tokenizer(input_text, return_tensors='pt').input_ids\n",
    "\n",
    "    # Prepare decoder input (start with BOS token)\n",
    "    decoder_input_ids = torch.tensor([[tokenizer.bos_token_id]])\n",
    "\n",
    "    # Iterative decoding\n",
    "    max_length = 50  # Define a reasonable maximum length\n",
    "    output_ids = decoder_input_ids\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, output_ids)\n",
    "        next_token_id = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(0)\n",
    "\n",
    "        # Break if EOS token is generated\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        # Append the new token to the sequence\n",
    "        output_ids = torch.cat([output_ids, next_token_id], dim=1)\n",
    "\n",
    "    # Decode the output tokens\n",
    "    output_text = tokenizer.decode(output_ids.squeeze(0), skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Input: {input_text}\")\n",
    "    print(f\"Output: {output_text}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    custom_config = BartConfig(\n",
    "        vocab_size=50265,  # Adjust according to your tokenizer\n",
    "        encoder_layers=4,  # Number of encoder layers\n",
    "        decoder_layers=4,  # Number of decoder layers\n",
    "        d_model=256,       # Dimensionality of the model\n",
    "        decoder_ffn_dim=1024,  # FFN size\n",
    "        encoder_ffn_dim=1024,\n",
    "        max_position_embeddings=512\n",
    "    )\n",
    "\n",
    "\n",
    "    run_inference(custom_config, model_weights_path=\"custom_bart_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
